{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa214aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69bcc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import VQModel\n",
    "from utils.dataset_highvars import get_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc37988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers import ModelMixin, ConfigMixin\n",
    "from diffusers.configuration_utils import register_to_config\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7878d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer1D(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.beta = beta # Commitment cost\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)\n",
    "\n",
    "    def forward(self, latents):\n",
    "        # latents shape: [Batch, Channels, Sequence_Length]\n",
    "        # Permute for embedding lookup: [B, L, C]\n",
    "        latents = latents.permute(0, 2, 1).contiguous()\n",
    "        flat_latents = latents.view(-1, self.embedding_dim)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_latents**2, dim=1, keepdim=True) \n",
    "                     + torch.sum(self.embedding.weight**2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_latents, self.embedding.weight.t()))\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=latents.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(latents.shape)\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), latents)\n",
    "        q_latent_loss = F.mse_loss(quantized, latents.detach())\n",
    "        loss = q_latent_loss + self.beta * e_latent_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = latents + (quantized - latents).detach()\n",
    "\n",
    "        # Permute back to [B, C, L]\n",
    "        quantized = quantized.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        return quantized, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9861c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Re-use the VectorQuantizer from the previous 1D example\n",
    "# (Assuming VectorQuantizer1D is available or pasted here)\n",
    "\n",
    "class TransformerVQModel(ModelMixin, ConfigMixin):\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features=2000,      # Input vector size (e.g., number of highly variable genes)\n",
    "        embedding_dim=256,      # Internal dimension for the Transformer\n",
    "        latent_dim=64,          # Dimension of the quantized latent vectors\n",
    "        n_heads=4,\n",
    "        n_layers=4,             # Depth of Encoder/Decoder\n",
    "        seq_len=32,             # We reshape input into a sequence of this length\n",
    "        num_embeddings=1024     # Codebook size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.feature_per_token = num_features // seq_len\n",
    "        \n",
    "        # Ensure divisible\n",
    "        assert num_features % seq_len == 0, \"num_features must be divisible by seq_len\"\n",
    "\n",
    "        # 1. Input Projection (flatten chunks of genes into tokens)\n",
    "        self.input_proj = nn.Linear(self.feature_per_token, embedding_dim)\n",
    "        \n",
    "        # 2. Learnable Positional Embeddings\n",
    "        # Essential for scRNA: tells the model \"This token contains Genes 0-100\"\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, seq_len, embedding_dim))\n",
    "\n",
    "        # 3. Transformer Encoder (Global Attention)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # 4. Pre-Quantization Projector\n",
    "        self.pre_quant = nn.Linear(embedding_dim, latent_dim)\n",
    "\n",
    "        # 5. Vector Quantizer\n",
    "        self.quantizer = VectorQuantizer1D(num_embeddings, latent_dim)\n",
    "\n",
    "        # 6. Post-Quantization Projector\n",
    "        self.post_quant = nn.Linear(latent_dim, embedding_dim)\n",
    "\n",
    "        # 7. Transformer Decoder\n",
    "        decoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=n_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # 8. Output Head\n",
    "        self.output_head = nn.Linear(embedding_dim, self.feature_per_token)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x shape: [Batch, Num_Features] (e.g., 2000 genes)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # 1. Reshape into Sequence: [B, Seq_Len, Features_Per_Token]\n",
    "        x = x.view(batch_size, self.seq_len, self.feature_per_token)\n",
    "        \n",
    "        # 2. Project to Embeddings & Add Position Info\n",
    "        h = self.input_proj(x) + self.pos_emb\n",
    "        \n",
    "        # 3. Transformer Pass (Global Mixing)\n",
    "        h = self.encoder(h)\n",
    "        \n",
    "        # 4. Project to Latent Dim\n",
    "        h = self.pre_quant(h)\n",
    "        \n",
    "        # 5. Quantize\n",
    "        # Note: Permute for Quantizer expects [B, Channels, Length] usually, \n",
    "        # but our custom 1D quantizer above expected [B, C, L]. \n",
    "        # Let's adjust based on the previous quantizer definition:\n",
    "        h = h.permute(0, 2, 1) # [B, Latent_Dim, Seq_Len]\n",
    "        quantized, loss = self.quantizer(h)\n",
    "        \n",
    "        return quantized, loss\n",
    "\n",
    "    def decode(self, quantized):\n",
    "        # quantized: [B, Latent_Dim, Seq_Len]\n",
    "        h = quantized.permute(0, 2, 1) # Back to [B, Seq_Len, Latent_Dim]\n",
    "        \n",
    "        h = self.post_quant(h) + self.pos_emb # Add pos emb again for decoder spatial awareness\n",
    "        h = self.decoder(h)\n",
    "        \n",
    "        # Project back to scalar values\n",
    "        out = self.output_head(h)\n",
    "        \n",
    "        # Flatten back to single vector: [B, Num_Features]\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        quantized, vq_loss = self.encode(x)\n",
    "        decoded = self.decode(quantized)\n",
    "        return decoded, vq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f25a317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, geneDim, maskidx = get_loader(num_samples = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2699c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerVQModel(\n",
    "    num_features=geneDim,    # Top 2000 Highly Variable Genes (HVGs)\n",
    "    seq_len=15,           # Split into 50 tokens (40 genes per token)\n",
    "    embedding_dim=256,\n",
    "    latent_dim=64,        # High compression\n",
    "    num_embeddings=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb4cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vcell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
