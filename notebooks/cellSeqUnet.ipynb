{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54373d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc876ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bdudas/anaconda3/envs/vcell/lib/python3.10/site-packages/pytorch_lightning/__init__.py:82: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.dataset_highvars import get_loader\n",
    "from dataclasses import dataclass\n",
    "import anndata as ad\n",
    "from torch.utils.data import Dataset\n",
    "import scanpy as sc\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27894951",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf68423",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    log_dir = \"logs\"\n",
    "    name = \"Highly_Var\"\n",
    "    batch_size = 64 # Genes processed at once\n",
    "    version = 1\n",
    "    epochs = 30\n",
    "    lr = 1e-3\n",
    "    num_workers = 15\n",
    "    num_samples = 700\n",
    "    target_gene_dim = 128\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82405597",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRoot = \"data/vcc_data\"\n",
    "tr_adata_path = f\"{dataRoot}/adata_Training.h5ad\"\n",
    "adata = ad.read_h5ad(tr_adata_path)\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "#sc.pp.highly_variable_genes(adata)\n",
    "#maskidx = (~adata.var.index.str.startswith(\"MT-\")) & adata.var.highly_variable\n",
    "#adata = adata[:,maskidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c96399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionDataset(Dataset):\n",
    "    def __init__(self, adata,batchMapping,seqLength = 32):\n",
    "        self.adata_batchX = adata.X\n",
    "        self.batch = adata.obs.batch.to_numpy()\n",
    "        self.batchmap = batchMapping\n",
    "        self.seqLength = seqLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.adata_batchX.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gene_expression = self.adata_batchX[idx].toarray().squeeze()\n",
    "        gene_expressionseq = gene_expression.reshape(self.seqLength,-1)\n",
    "        origin =  self.batchmap[self.batch[idx]]\n",
    "        return gene_expressionseq, origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a447bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchMapping = {b:i for i,b in enumerate(adata.obs.batch.unique())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10abbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GeneExpressionDataset(adata[adata.obs.target_gene == \"non-targeting\"],batchMapping)\n",
    "trainDataset,valDataset = torch.utils.data.random_split(dataset,[int(0.8*len(dataset)),len(dataset)-int(0.8*len(dataset))])\n",
    "#implement k-fold later\n",
    "train_loader = torch.utils.data.DataLoader(trainDataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(valDataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59160447",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes, batchinfo = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf754516",
   "metadata": {},
   "source": [
    "# Design Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "qLayer = nn.Linear(565,64)\n",
    "kLayer = nn.Linear(565,64)\n",
    "vLayer = nn.Linear(565,64)\n",
    "q = qLayer(genes)\n",
    "k = kLayer(genes)\n",
    "v = vLayer(genes)\n",
    "multiAttention = nn.MultiheadAttention(embed_dim=64,num_heads = 8)\n",
    "latent, grad= multiAttention(q,k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63939edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpAttention(nn.Module):\n",
    "    def __init__(self, inDims, outDims):\n",
    "        super().__init__()\n",
    "        self.expand_layer = nn.Linear(inDims, outDims)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=outDims, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(outDims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.expand_layer(x)\n",
    "        x, _ = self.mha(x, x, x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, latentDim, seq_len=32):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # 1. Inverse of the Encoder's projection\n",
    "        # We map z back to (Seq_Len * Smallest_Feature_Dim)\n",
    "        # Based on your encoder, the smallest dim was 64.\n",
    "        self.linear_map = nn.Linear(latentDim, 64 * seq_len)\n",
    "        \n",
    "        # 2. Inverse of the DownAttention layers\n",
    "        self.up3 = UpAttention(inDims=64, outDims=128)\n",
    "        self.up2 = UpAttention(inDims=128, outDims=256)\n",
    "        self.up1 = UpAttention(inDims=256, outDims=565) # Final dimension matches input\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # z: [Batch, latentDim]\n",
    "        \n",
    "        # 1. Project and Unflatten\n",
    "        x = self.linear_map(z)\n",
    "        # Reshape to [Batch, Seq_Len, 64]\n",
    "        x = x.view(-1, self.seq_len, 64) \n",
    "        \n",
    "        # 2. Expand features back up\n",
    "        x = self.up3(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e281fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self,recon_x,x,mu,logvar):\n",
    "        recon_loss = self.mse(recon_x,x)\n",
    "        kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + kld_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f25c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DownAttention(nn.Module):\n",
    "    def __init__(self,inDims,outDims):\n",
    "        super().__init__()\n",
    "        self.qlayer = nn.Linear(inDims,outDims)\n",
    "        self.klayer = nn.Linear(inDims,outDims)\n",
    "        self.vlayer = nn.Linear(inDims,outDims)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim = outDims,num_heads=8,batch_first=True) # Might be scaled\n",
    "        self.norm = nn.LayerNorm(outDims)\n",
    "    def forward(self,x):\n",
    "        q = self.qlayer(x)\n",
    "        k = self.klayer(x)\n",
    "        v = self.vlayer(x)\n",
    "        attn,_ = self.mha(q,k,v)\n",
    "        attn = self.norm(attn)\n",
    "        return attn\n",
    "    \n",
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self,latentDim):\n",
    "        super().__init__()\n",
    "        self.down1 = DownAttention(inDims = 565,outDims=256)\n",
    "        self.down2 = DownAttention(inDims = 256,outDims=128)\n",
    "        self.down3 = DownAttention(inDims = 128,outDims=64)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.proj = nn.Linear(64*32,latentDim*2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        x = self.proj(self.flat(x))\n",
    "        return torch.chunk(x,chunks = 2,dim = 1)\n",
    "    \n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self,latentDim,numseqs):\n",
    "        super().__init__()\n",
    "        self.numseqs = numseqs\n",
    "        self.unflatten = nn.Linear(latentDim,numseqs*64)\n",
    "        self.up1 = DownAttention(inDims = 64,outDims = 128)\n",
    "        self.up2 = DownAttention(inDims = 128,outDims = 256)\n",
    "        self.up3 = DownAttention(inDims = 256,outDims = 512)\n",
    "        self.final = nn.Linear(512,565)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.unflatten(x)\n",
    "        x = x.view(-1,self.numseqs,64)\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        return self.final(x)\n",
    "\n",
    "class AttentionVae(pl.LightningModule):\n",
    "    def __init__(self,latentDim):\n",
    "        super().__init__()\n",
    "        self.encoder = AttentionEncoder(latentDim)\n",
    "        self.decoder = AttentionDecoder(latentDim,32)\n",
    "        self.criterion = MyLoss() \n",
    "    \n",
    "    def reparametrise(self,mu,logvar,mode:str=\"train\"):\n",
    "        if mode  == \"train\":\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "    \n",
    "    def shared_step(self,batch,mode):\n",
    "        genes, _ = batch\n",
    "        mu,logvar = self.encoder(genes)\n",
    "        z = self.reparametrise(mu,logvar,mode)\n",
    "        x = self.decoder(z)\n",
    "        return genes,x,mu,logvar\n",
    "    \n",
    "    def training_step(self, batch,batch_idx):\n",
    "        genes, x,mu,logvar = self.shared_step(batch,mode=\"train\")\n",
    "        loss = self.criterion(x, genes, mu, logvar)\n",
    "        self.log(\"train/loss\", loss,prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        genes, x,mu,logvar = self.shared_step(batch,mode=\"val\")\n",
    "        loss = self.criterion(x, genes, mu, logvar)\n",
    "        self.log(\"val/loss\", loss,prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d22beb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionVae(latentDim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1c268c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/home/bdudas/anaconda3/envs/vcell/lib/python3.10/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | AttentionEncoder | 3.0 M \n",
      "1 | decoder   | AttentionDecoder | 3.2 M \n",
      "2 | criterion | MyLoss           | 0     \n",
      "-----------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  29%|██▊       | 171/598 [00:20<00:50,  8.53it/s, loss=0.921, v_num=2, val/loss=0.915, train/loss=0.922]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bdudas/anaconda3/envs/vcell/lib/python3.10/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  29%|██▊       | 171/598 [00:21<00:53,  8.05it/s, loss=0.921, v_num=2, val/loss=0.915, train/loss=0.922]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=cfg.epochs)\n",
    "trainer.fit(model,train_loader,val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f14d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27aa37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vcell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
