{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4453eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0efa05e",
   "metadata": {},
   "source": [
    "# Create Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7142e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.Bence.dataset import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11cba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    log_dir = \"logs\"\n",
    "    name = \"PosPerturb\"\n",
    "    batch_size = 128 # Genes processed at once\n",
    "    version = 1\n",
    "    epochs = 30\n",
    "    lr = 1e-3\n",
    "    num_workers = 15\n",
    "    num_samples = 100\n",
    "    target_gene_dim = 128\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449da990",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,gene_dim,_ = get_loader(cfg.num_samples,cfg.target_gene_dim)\n",
    "trainDataset,valDataset = torch.utils.data.random_split(dataset,[int(0.8*len(dataset)),len(dataset)-int(0.8*len(dataset))])\n",
    "#implement k-fold later\n",
    "train_loader = torch.utils.data.DataLoader(trainDataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(valDataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ee38e",
   "metadata": {},
   "source": [
    "# Train pipeline desings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3c98581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet1DModel, DDPMScheduler\n",
    "import lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eff0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchInfo, geneExpression, perturb = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eed0dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 256 # Length of geneExpression -> probably need a VAE to encode\n",
    "DATA_CHANNELS = 1\n",
    "COND_CHANNELS = 128 # Perturbation embedding \n",
    "TOTAL_CHANNELS = DATA_CHANNELS + COND_CHANNELS\n",
    "\n",
    "model = UNet1DModel(\n",
    "    sample_size=SEQ_LENGTH,      \n",
    "    in_channels=TOTAL_CHANNELS,        # 1 \"channel\" of data\n",
    "    out_channels=DATA_CHANNELS,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256),\n",
    "    down_block_types=(\"DownBlock1D\", \"DownBlock1D\", \"AttnDownBlock1D\"),\n",
    "    up_block_types=(\"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1D\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_gene = geneExpression[:,None,:SEQ_LENGTH]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6e6bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3868249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_gene = geneExpression[:,None,:SEQ_LENGTH] # INPUT DATA\n",
    "timesteps = torch.randint(0, 1000, (cfg.batch_size,)).long()\n",
    "cond_expanded = perturb.unsqueeze(-1).expand(-1, -1, SEQ_LENGTH)\n",
    "\n",
    "model_input = torch.cat([latent_gene,cond_expanded],dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1e4c7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_pred = model(model_input, timesteps).sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e6e4a",
   "metadata": {},
   "source": [
    "# Def train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2d8c7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        # KL divergence loss\n",
    "        #self.criterion = F.kl_div\n",
    "        self.criterion = torch.nn.MSELoss() # Wasserstein loss\n",
    "    \n",
    "    def __call__(self, outputs, targets):\n",
    "        #log_probs = F.log_softmax(outputs, dim=-1)\n",
    "        #targets_probs = F.softmax(targets, dim=-1)\n",
    "        #self.criterion(log_probs, targets_probs, reduction='batchmean')\n",
    "        return self.criterion(outputs, targets)\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d82992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDiffusion(pl.LightningModule):\n",
    "    def __init__(self,criterion):\n",
    "        super().__init__()\n",
    "        self.model = UNet1DModel(\n",
    "            sample_size=SEQ_LENGTH,      \n",
    "            in_channels=TOTAL_CHANNELS,        # 1 \"channel\" of data\n",
    "            out_channels=DATA_CHANNELS,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(64, 128, 256),\n",
    "            down_block_types=(\"DownBlock1D\", \"DownBlock1D\", \"AttnDownBlock1D\"),\n",
    "            up_block_types=(\"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1D\"),\n",
    "            )\n",
    "        self.criterion = criterion\n",
    "        self.lossName = criterion.get_name()\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def shared_step(self,latent_gene,perturb,mode):\n",
    "        cond_expanded = perturb.unsqueeze(-1).expand(-1, -1, SEQ_LENGTH)\n",
    "        timesteps = torch.randint(0, 1000, (cfg.batch_size,)).long()\n",
    "        model_input = torch.cat([latent_gene,cond_expanded],dim = 1)\n",
    "        noise = torch.randn_like(model_input)\n",
    "        noisy_data = scheduler.add_noise(model_input, noise, timesteps)\n",
    "        noise_pred = self.model(noisy_data, timesteps).sample\n",
    "        loss = self.criterion(noise_pred,noise)\n",
    "        self.log(f\"{mode}/loss\",loss,prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        self.model.train()\n",
    "        _, geneExpression, perturb = batch\n",
    "        latentGene = geneExpression[:,None,:SEQ_LENGTH]\n",
    "        loss = self.shared_step(latentGene,perturb,\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        self.model.eval()\n",
    "        _, geneExpression, perturb = batch\n",
    "        latentGene = geneExpression[:,None,:SEQ_LENGTH]\n",
    "        loss = self.shared_step(latentGene,perturb,\"val\")\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9d1d85d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs = 1, accelerator=\"cpu\",default_root_dir=\"testing\")\n",
    "model = GeneDiffusion(criterion = MyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e67c3531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model | UNet1DModel | 10.5 M | train\n",
      "----------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "8         Non-trainable params\n",
      "10.5 M    Total params\n",
      "41.921    Total estimated model params size (MB)\n",
      "289       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|â–Ž         | 40/1145 [01:37<44:48,  0.41it/s, v_num=5, train/loss=1.050]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,train_loader,val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vcell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4453eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0efa05e",
   "metadata": {},
   "source": [
    "# Create Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7142e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicsnemo.models.mlp.fully_connected import FullyConnected\n",
    "from physicsnemo.models.diffusion import SongUNet\n",
    "import torch\n",
    "from utils.datasetBence import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11cba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    log_dir = \"logs\"\n",
    "    name = \"PosPerturb\"\n",
    "    batch_size = 128 # Genes processed at once\n",
    "    version = 1\n",
    "    epochs = 30\n",
    "    lr = 1e-3\n",
    "    num_workers = 15\n",
    "    num_samples = 100\n",
    "    target_gene_dim = 128\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449da990",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,gene_dim,_ = get_loader(cfg.num_samples,cfg.target_gene_dim)\n",
    "trainDataset,valDataset = torch.utils.data.random_split(dataset,[int(0.8*len(dataset)),len(dataset)-int(0.8*len(dataset))])\n",
    "#implement k-fold later\n",
    "train_loader = torch.utils.data.DataLoader(trainDataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(valDataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ee38e",
   "metadata": {},
   "source": [
    "# Train pipeline desings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3c98581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet1DModel, DDPMScheduler\n",
    "import lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eff0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchInfo, geneExpression, perturb = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eed0dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 256 # Length of geneExpression -> probably need a VAE to encode\n",
    "DATA_CHANNELS = 1\n",
    "COND_CHANNELS = 128 # Perturbation embedding \n",
    "TOTAL_CHANNELS = DATA_CHANNELS + COND_CHANNELS\n",
    "\n",
    "model = UNet1DModel(\n",
    "    sample_size=SEQ_LENGTH,      \n",
    "    in_channels=TOTAL_CHANNELS,        # 1 \"channel\" of data\n",
    "    out_channels=DATA_CHANNELS,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256),\n",
    "    down_block_types=(\"DownBlock1D\", \"DownBlock1D\", \"AttnDownBlock1D\"),\n",
    "    up_block_types=(\"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1D\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_gene = geneExpression[:,None,:SEQ_LENGTH]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6e6bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3868249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_gene = geneExpression[:,None,:SEQ_LENGTH] # INPUT DATA\n",
    "timesteps = torch.randint(0, 1000, (cfg.batch_size,)).long()\n",
    "cond_expanded = perturb.unsqueeze(-1).expand(-1, -1, SEQ_LENGTH)\n",
    "\n",
    "model_input = torch.cat([latent_gene,cond_expanded],dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1e4c7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_pred = model(model_input, timesteps).sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e6e4a",
   "metadata": {},
   "source": [
    "# Def train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2d8c7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyLoss():\n",
    "    def __init__(self):\n",
    "        # KL divergence loss\n",
    "        #self.criterion = F.kl_div\n",
    "        self.criterion = torch.nn.MSELoss() # Wasserstein loss\n",
    "    \n",
    "    def __call__(self, outputs, targets):\n",
    "        #log_probs = F.log_softmax(outputs, dim=-1)\n",
    "        #targets_probs = F.softmax(targets, dim=-1)\n",
    "        #self.criterion(log_probs, targets_probs, reduction='batchmean')\n",
    "        return self.criterion(outputs, targets)\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d82992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDiffusion(pl.LightningModule):\n",
    "    def __init__(self,criterion):\n",
    "        super().__init__()\n",
    "        self.model = UNet1DModel(\n",
    "            sample_size=SEQ_LENGTH,      \n",
    "            in_channels=TOTAL_CHANNELS,        # 1 \"channel\" of data\n",
    "            out_channels=DATA_CHANNELS,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(64, 128, 256),\n",
    "            down_block_types=(\"DownBlock1D\", \"DownBlock1D\", \"AttnDownBlock1D\"),\n",
    "            up_block_types=(\"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1D\"),\n",
    "            )\n",
    "        self.criterion = criterion\n",
    "        self.lossName = criterion.get_name()\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def shared_step(self,latent_gene,perturb,mode):\n",
    "        cond_expanded = perturb.unsqueeze(-1).expand(-1, -1, SEQ_LENGTH)\n",
    "        timesteps = torch.randint(0, 1000, (cfg.batch_size,)).long()\n",
    "        model_input = torch.cat([latent_gene,cond_expanded],dim = 1)\n",
    "        noise = torch.randn_like(model_input)\n",
    "        noisy_data = scheduler.add_noise(model_input, noise, timesteps)\n",
    "        noise_pred = self.model(noisy_data, timesteps).sample\n",
    "        loss = self.criterion(noise_pred,noise)\n",
    "        self.log(f\"{mode}/loss\",loss,prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        self.model.train()\n",
    "        _, geneExpression, perturb = batch\n",
    "        latentGene = geneExpression[:,None,:SEQ_LENGTH]\n",
    "        loss = self.shared_step(latentGene,perturb,\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        self.model.eval()\n",
    "        _, geneExpression, perturb = batch\n",
    "        latentGene = geneExpression[:,None,:SEQ_LENGTH]\n",
    "        loss = self.shared_step(latentGene,perturb,\"val\")\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9d1d85d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs = 1, accelerator=\"cpu\",default_root_dir=\"testing\")\n",
    "model = GeneDiffusion(criterion = MyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e67c3531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model | UNet1DModel | 10.5 M | train\n",
      "----------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "8         Non-trainable params\n",
      "10.5 M    Total params\n",
      "41.921    Total estimated model params size (MB)\n",
      "289       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|â–Ž         | 40/1145 [01:37<44:48,  0.41it/s, v_num=5, train/loss=1.050]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,train_loader,val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vcell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
